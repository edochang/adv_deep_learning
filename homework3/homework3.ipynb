{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ba04a1",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd0b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val grader loaded.\n",
      "\u001b[37m[INFO     00:00:844] \u001b[0m\u001b[37mModel non-batched inference grader\u001b[0m\n",
      "2025-11-03 21:49:00.365804: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-03 21:49:00.443760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 21:49:01.821175: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "100%|███████████████████████████████████████████| 32/32 [00:11<00:00,  2.78it/s]\n",
      "\u001b[33m[WARNING  00:19:760] \u001b[0m\u001b[33m  - Test non-batched generate function                 [ 10 / 10  ]\u001b[0m\n",
      "\u001b[37m[INFO     00:19:760] \u001b[0m\u001b[37m --------------------------------------------------    [  10 /  10 ]\u001b[0m\n",
      "\u001b[37m[INFO     00:19:761] \u001b[0m\u001b[37mModel batched inference grader\u001b[0m\n",
      "\u001b[33m[WARNING  00:29:814] \u001b[0m\u001b[33m  - Test batched generate function                     [ 15 / 15  ]\u001b[0m\n",
      "\u001b[37m[INFO     00:29:814] \u001b[0m\u001b[37m --------------------------------------------------    [  15 /  15 ]\u001b[0m\n",
      "\u001b[37m[INFO     00:29:815] \u001b[0m\u001b[37mCoT Model Grader\u001b[0m\n",
      "LLM Running on Micro Batches 32: 100%|████████████| 4/4 [00:29<00:00,  7.40s/it]\n",
      "\u001b[33m[WARNING  00:59:612] \u001b[0m\u001b[33m  - Test the answer accuracy                           [ 25 / 25  ]\u001b[0m\n",
      "\u001b[37m[INFO     00:59:612] \u001b[0m\u001b[37m --------------------------------------------------    [  25 /  25 ]\u001b[0m\n",
      "\u001b[37m[INFO     00:59:613] \u001b[0m\u001b[37mSFT Model Grader\u001b[0m\n",
      "LLM Running on Micro Batches 32: 100%|████████████| 4/4 [00:06<00:00,  1.72s/it]\n",
      "\u001b[33m[WARNING  01:07:518] \u001b[0m\u001b[33m  - Test the answer accuracy                           [ 25 / 25  ]\u001b[0m\n",
      "\u001b[37m[INFO     01:07:518] \u001b[0m\u001b[37m --------------------------------------------------    [  25 /  25 ]\u001b[0m\n",
      "\u001b[37m[INFO     01:07:518] \u001b[0m\u001b[37mRFT Model Grader\u001b[0m\n",
      "\u001b[33m[WARNING  01:08:874] \u001b[0m\u001b[33m  - Test the answer accuracy                           [ 0 / 25 ValueError ]\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/miniconda3/envs/deeplearn/lib/python3.11/site-packages/peft/config.py\", line 262, in _get_peft_type\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    config_file = hf_hub_download(\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m                  ^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/miniconda3/envs/deeplearn/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    validate_repo_id(arg_value)\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/miniconda3/envs/deeplearn/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 151, in validate_repo_id\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    raise HFValidationError(f\"Repo id must be a string, not {type(repo_id)}: '{repo_id}'.\")\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31mhuggingface_hub.errors.HFValidationError: Repo id must be a string, not <class 'pathlib.PosixPath'>: '/home/edo/repo/adv_deep_learning/homework3/homework/rft_model'.\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/repo/adv_deep_learning/homework3/grader/grader.py\", line 64, in wrapper\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    v = func(self, **a)\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m        ^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/repo/adv_deep_learning/homework3/grader/tests.py\", line 125, in test_validation_loss\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    model = self.load_model()\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m            ^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/repo/adv_deep_learning/homework3/grader/tests.py\", line 105, in load_model\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    llm = getattr(self.module, f\"load_{self.model_name}\")()\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/repo/adv_deep_learning/homework3/homework/rft.py\", line 14, in load\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    llm.model = PeftModel.from_pretrained(llm.model, model_path).to(llm.device)\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/miniconda3/envs/deeplearn/lib/python3.11/site-packages/peft/peft_model.py\", line 440, in from_pretrained\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    PeftConfig._get_peft_type(\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m  File \"/home/edo/miniconda3/envs/deeplearn/lib/python3.11/site-packages/peft/config.py\", line 268, in _get_peft_type\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31m    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{model_id}'\")\u001b[0m\n",
      "\u001b[31m[ERROR    01:08:875] \u001b[0m\u001b[31mValueError: Can't find 'adapter_config.json' at '/home/edo/repo/adv_deep_learning/homework3/homework/rft_model'\u001b[0m\n",
      "\u001b[37m[INFO     01:08:875] \u001b[0m\u001b[37m --------------------------------------------------    [   0 /  25 ]\u001b[0m\n",
      "\u001b[37m[INFO     01:08:875] \u001b[0m\u001b[37mTotal                                                     75 / 100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m grader homework -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb7a0e",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1\n",
    "\n",
    "Note: use `!` magic command to execute a shell command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38db751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'homework.base_llm' found in sys.modules after import of package 'homework', but prior to execution of 'homework.base_llm'; this may result in unpredictable behaviour\n",
      "2025-11-03 21:34:39.822951: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-03 21:34:40.477298: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 21:34:42.636321: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "BaseLLM __initi__: Loaded model on device: cuda\n",
      "testing generate function\n",
      "input The cat went up\n",
      "output  the stairs and then down again.\n",
      "\n",
      "What is wrong with this sentence?\n",
      "testing generate function\n",
      "input The dog went down\n",
      "output  the stairs and into the basement.\n",
      "\n",
      "Both sentences are grammatically correct, but they have slightly different meanings. The first sentence implies that the dog is going upstairs (downstairs) to the basement, while the second sentence suggests that the dog is going downstairs (upstairs).\n",
      "\n",
      "In general, if you're describing a movement from one location to another, use \"down\" for a downward motion, and \"up\" for an upward motion. If you're describing a movement from one place to another in relation to a specific direction or level, use \"down\" for a downward motion and \"up\" for an upward motion.\n",
      "[' the tree.\\n\\nBoth sentences are grammatically correct, but they have different meanings and uses. The first sentence implies that the cat climbed up a tree, while the second sentence suggests that the cat was already in the tree before climbing.\\n\\nTo clarify this difference, consider the following:\\n\\n1. If you\\'re describing an action that occurred after something else happened (like \"after I ate breakfast\"), then \"went\" is often used to indicate the time of day or the duration of the action. For example:\\n   - After I finished my homework, I went to bed.\\n   - She walked through the park, going for a run.\\n\\n2. If you\\'re describing an action that occurred at the same time as another event (', ' the stairs and into the basement.\\n\\nIn this sentence, \"down\" is an adverb modifying the verb \"went\". It indicates the direction of the action (going downstairs).\\n\\nNow let\\'s look at another example:\\n\\n* The cat sat on the mat.\\n* The cat was sitting on the mat.\\n\\nIn these sentences, \"sat\" is a verb that describes the state of the cat. It tells us what happened to the cat (it was sitting).\\n\\nSo, while both words can modify verbs, they have different functions in describing actions.']\n"
     ]
    }
   ],
   "source": [
    "!python -m homework.base_llm test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d19f9",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eba9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'homework.cot' found in sys.modules after import of package 'homework', but prior to execution of 'homework.cot'; this may result in unpredictable behaviour\n",
      "2025-11-03 21:35:40.333336: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-03 21:35:40.406705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 21:35:41.912757: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "BaseLLM __initi__: Loaded model on device: cuda\n",
      "LLM Running on Micro Batches 32: 100%|████████████| 4/4 [00:35<00:00,  8.81s/it]\n",
      "BaseLLM parse_answer: BAD answer:\n",
      "Conversion used: 1 meter/second/kilometer per hour\n",
      "Calculation: 4 * (1000 / 3600) = 4 * 0.66666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666\n",
      "benchmark_result.accuracy=0.54  benchmark_result.answer_rate=0.99\n"
     ]
    }
   ],
   "source": [
    "!python -m homework.cot test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a40eb",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b78fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'homework.sft' found in sys.modules after import of package 'homework', but prior to execution of 'homework.sft'; this may result in unpredictable behaviour\n",
      "2025-11-03 21:38:04.017479: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-03 21:38:04.096997: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 21:38:05.501929: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "BaseLLM __initi__: Loaded model on device: cuda\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "  0%|                                                   | 0/160 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "{'train_runtime': 439.7201, 'train_samples_per_second': 11.371, 'train_steps_per_second': 0.364, 'train_loss': 0.2720987319946289, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 160/160 [07:19<00:00,  2.75s/it]\n",
      "BaseLLM __initi__: Loaded model on device: cuda\n",
      "LLM Running on Micro Batches 32: 100%|████████████| 4/4 [00:08<00:00,  2.12s/it]\n",
      "benchmark_result.accuracy=0.67  benchmark_result.answer_rate=0.99\n"
     ]
    }
   ],
   "source": [
    "!python -m homework.sft train homework/sft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926d821",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be770e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-03 21:25:58.225643: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-03 21:25:58.890401: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-03 21:26:02.407111: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Generate rft dataset with datagen\n",
    "#!python -m homework.datagen --output_json data/rft.json --oversample 10 --temperature 0.6\n",
    "!python -m homework.datagen --output_json data/rft.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ae6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m homework.sft train homework/rft_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

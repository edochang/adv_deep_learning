{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bb7a0e",
   "metadata": {},
   "source": [
    "Token Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f38db751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edo/miniconda3/envs/deeplearn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 36\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokens are a model's context window / memory.  The model can only \"see\" this many tokens at a time.\n",
    "# different models have different context window sizes.\n",
    "# smaller models tend to have smaller context windows.\n",
    "# larger models tend to have larger context windows.\n",
    "# context window sizes can range from 512 tokens to 32768 tokens or more.\n",
    "# for reference, 1 token is approximately 4 characters of English text, so 1000 tokens is approximately 750 words.\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant specialized in converting a unit to another unit.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the conversion of 10 miles to kilometers?\"},\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "tokens = tokenizer(input_text).input_ids\n",
    "print(f\"Token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eba9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceTB/SmolLM2-360M-Instruct tokenizer.model_max_length: 8192\n",
      "HuggingFaceTB/SmolLM2-360M-Instruct config.max_position_embeddings: 8192\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "\n",
    "# HuggingFaceTB/SmolLM2-360M-Instruct has a context window of 2048 tokens\n",
    "print(f\"HuggingFaceTB/SmolLM2-360M-Instruct tokenizer.model_max_length: {tokenizer.model_max_length}\")\n",
    "print(f\"HuggingFaceTB/SmolLM2-360M-Instruct config.max_position_embeddings: {config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a40eb",
   "metadata": {},
   "source": [
    "Question and Answer Sampling from Model\n",
    "\n",
    "https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b78fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def model_generate(messages, num_return_sequences: int | None = None, temperature: float = 0, max_new_tokens=50, repetition_penalty=None):\n",
    "    checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "\n",
    "    device = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    # for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    tokens = tokenizer(input_text).input_ids\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    #print(input_text)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    do_sample = True if temperature > 0 else False\n",
    "    n_return_sequences = num_return_sequences if num_return_sequences is not None else 1\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature if do_sample else 1.0,\n",
    "        num_return_sequences=n_return_sequences, \n",
    "        repetition_penalty=repetition_penalty if do_sample else None,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    #print(outputs)\n",
    "\n",
    "    #print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "    # decoded_outputs is a list of strings corresponding to each generated output\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    if n_return_sequences == 1:\n",
    "        # only one output, print it directly from the list of strings\n",
    "        print(decoded_outputs[0])\n",
    "    else:\n",
    "        for i, output in enumerate(decoded_outputs):\n",
    "            print(f\"--- **Result {i+1}** --------------------\")\n",
    "            print(output)\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ec7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant specialized in unit conversions and will be answering a unit conversion question or request. Follow these rules when responding:\\n\"\n",
    "    \"- Be concise and wrap the numerical result in <answer> tags. Example: <answer>5.86</answer>\\n\"\n",
    "    \"- Double check your calculations\\n\"\n",
    ")\n",
    "\n",
    "# hyperparameters\n",
    "temp = 0.6\n",
    "repetition_penalty = 1.1  # override repetition penalty to reduce repetitive outputs\n",
    "# Set a reasonable limit for the number of tokens to generate. Prevents infinite generation.\n",
    "max_new_tokens = 150\n",
    "num_return_sequences = 10 # number of samples to generate\n",
    "\n",
    "question = \"How do we translate 7 KB into bit?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2448c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 79\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m messages = [\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: system_prompt},\n\u001b[32m      3\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question},\n\u001b[32m      4\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmodel_generate\u001b[39m\u001b[34m(messages, num_return_sequences, temperature, max_new_tokens, repetition_penalty)\u001b[39m\n\u001b[32m     17\u001b[39m do_sample = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m temperature > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     18\u001b[39m n_return_sequences = num_return_sequences \u001b[38;5;28;01mif\u001b[39;00m num_return_sequences \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m outputs = model.generate(\n\u001b[32m     21\u001b[39m     **inputs, \n\u001b[32m     22\u001b[39m     max_new_tokens=max_new_tokens,\n\u001b[32m     23\u001b[39m     do_sample=do_sample,\n\u001b[32m     24\u001b[39m     temperature=temperature \u001b[38;5;28;01mif\u001b[39;00m do_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1.0\u001b[39m,\n\u001b[32m     25\u001b[39m     num_return_sequences=n_return_sequences, \n\u001b[32m     26\u001b[39m     repetition_penalty=repetition_penalty \u001b[38;5;28;01mif\u001b[39;00m do_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m     eos_token_id=tokenizer.eos_token_id\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#print(tokenizer.decode(outputs[0], skip_special_tokens=True))\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# decoded_outputs is a list of strings corresponding to each generated output\u001b[39;00m\n\u001b[32m     35\u001b[39m decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "model_generate(\n",
    "    messages,\n",
    "    temperature=temp,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    num_return_sequences=num_return_sequences,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb53a97",
   "metadata": {},
   "source": [
    "Use few shot examples:\n",
    "\n",
    "Example 1: Convert 10 kilometers to miles. 1 kilometer = 0.621371 miles. Answer: 6.21371 miles.\n",
    "\n",
    "Example 2: Convert 2 liters to gallons. 1 liter = 0.264172 gallons. Answer: 0.528344 gallons.\n",
    "\n",
    "Now, convert 5 meters to feet. 1 meter = 3.28084 feet. Answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28223a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 501\n",
      "system\n",
      "You are an assistant specialized in unit conversions and will be answering a unit conversion question or request. Follow these rules when responding:\n",
      "- Be concise and wrap the numerical result in <answer> tags. Example: <answer>5.86</answer>\n",
      "- Double check your calculations\n",
      "\n",
      "user\n",
      "Can you change 9 metric ton to its equivalent in kg?\n",
      "assistant\n",
      "Conversion calculation: 9 metric ton * 1000 kg/metric ton = 9000 kg\n",
      "<answer>9000.0</answer>\n",
      "user\n",
      "What is the conversion of 4 mph to ft/s?\n",
      "assistant\n",
      "Conversion calculation: 4 mph * 5280 ft/mile * (1 hour / 3600 seconds) = 5.867 ft/s\n",
      "<answer>5.867</answer>\n",
      "user\n",
      "Convert the measurement of 5 kB into bit.\n",
      "assistant\n",
      "Conversion calculation: 5 kB * 1024 bytes/kB * 8 bits/byte = 40960 bits\n",
      "<answer>40960.0</answer>\n",
      "user\n",
      "Can you provide the conversion value from years to week for 7 units?\n",
      "assistant\n",
      "Conversion calculation: 7 years * 365 days/year * 1 week/7 days = 364.24219878125 weeks\n",
      "<answer>364.24219878125</answer>\n",
      "user\n",
      "Please convert 1 ft into cm.\n",
      "assistant\n",
      "Conversion calculation: 1 ft * 12 in/ft *  2.54 cm/in = 30.48 cm\n",
      "<answer>30.48</answer>\n",
      "user\n",
      "What is the equivalent of 9 milliliter in mm^3?\n",
      "assistant\n",
      "Conversion calculation: 9 milliliter * 1000 mm^3/milliliter = 9000 mm^3\n",
      "<answer>9000</answer>\n",
      "user\n",
      "How do we translate 7 KB into bit?\n",
      "assistant\n",
      "Conversion calculation: 7 kb * 1024 bytes/kb * 8 bits/byte = 58672 bits\n",
      "<answer>58672.0</answer>\n"
     ]
    }
   ],
   "source": [
    "messages2 = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you change 9 metric ton to its equivalent in kg?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Conversion calculation: 9 metric ton * 1000 kg/metric ton = 9000 kg\\n\"\n",
    "            \"<answer>9000.0</answer>\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the conversion of 4 mph to ft/s?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Conversion calculation: 4 mph * 5280 ft/mile * (1 hour / 3600 seconds) = 5.867 ft/s\\n\"\n",
    "            \"<answer>5.867</answer>\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Convert the measurement of 5 kB into bit.\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Conversion calculation: 5 kB * 1024 bytes/kB * 8 bits/byte = 40960 bits\\n\"\n",
    "            \"<answer>40960.0</answer>\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you provide the conversion value from years to week for 7 units?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Conversion calculation: 7 years * 365 days/year * 1 week/7 days = 364.24219878125 weeks\\n\"\n",
    "            \"<answer>364.24219878125</answer>\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Please convert 1 ft into cm.\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Conversion calculation: 1 ft * 12 in/ft *  2.54 cm/in = 30.48 cm\\n\"\n",
    "            \"<answer>30.48</answer>\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"What is the equivalent of 9 milliliter in mm^3?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"Conversion calculation: 9 milliliter * 1000 mm^3/milliliter = 9000 mm^3\\n\"\n",
    "            \"<answer>9000</answer>\"\n",
    "        ),\n",
    "    },\n",
    "    # the actual question\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "model_generate(\n",
    "    messages2,\n",
    "    temperature=temp,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a4426",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "Function to test implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153ceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseLLM __initi__: Loaded model on device: cuda using model (HuggingFaceTB/SmolLM2-360M-Instruct) with float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM Running on Micro Batches 32:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from homework.datagen import *\n",
    "from homework.cot import *\n",
    "from homework.data import *\n",
    "\n",
    "trainset = Dataset(\"train\") # about a 1000 train samples\n",
    "trainset = trainset[:64] # get 64 samples for quick testing\n",
    "\n",
    "oversample = 10\n",
    "temperature = 0.6\n",
    "\n",
    "model = CoTModel()\n",
    "\n",
    "trainset_size = range(len(trainset))\n",
    "prompts = [model.format_prompt(trainset[i][0]) for i in trainset_size]\n",
    "\n",
    "generations = model.batched_generate(prompts, num_return_sequences=oversample, temperature=temperature)\n",
    "\n",
    "if isinstance(generations[0], list):\n",
    "    # multiple generations per prompt\n",
    "    print(\"=== Generations ===\")\n",
    "    for i, gen in enumerate(generations):\n",
    "        for j, g in enumerate(gen):\n",
    "            print(f\"Prompt {i + 1}, Sample {j + 1}: \\n {g}\")\n",
    "elif isinstance(generations[0], str):\n",
    "    # single generation per prompt\n",
    "    for i, g in enumerate(generations):\n",
    "        print(f\"Prompt {i + 1}: \\n {g}\")\n",
    "else:\n",
    "    # unexpected format\n",
    "    print(\"Unexpected generation format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
